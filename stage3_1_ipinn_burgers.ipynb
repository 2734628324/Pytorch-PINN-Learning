{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c30d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f7747a",
   "metadata": {},
   "source": [
    "## 5. æ ¸å¿ƒæ¨¡å¼ (Core Pattern)\n",
    "\n",
    "### 5.1 æ··åˆé©±åŠ¨çš„ä¼˜åŒ– (Hybrid-Driven Optimization)\n",
    "\n",
    "| é©±åŠ¨åŠ› | è§’è‰² | ä½œç”¨æ–¹å¼ |\n",
    "|---|---|---|\n",
    "| **Data-Driven** | å·²çŸ¥çš„ç¨€ç–è§‚æµ‹ç‚¹ | é€šè¿‡ `loss_data = loss_fn(u_pred, u_label)` ä¿è¯æ¨¡å‹**æ‹Ÿåˆè§‚æµ‹æ•°æ®** |\n",
    "| **Physics-Informed** | æ— å¤„ä¸åœ¨çš„ PDE | é€šè¿‡ `loss_pde = loss_fn(residual, 0)` å¼ºåˆ¶æ¨¡å‹**éµå®ˆç‰©ç†å®šå¾‹** |\n",
    "\n",
    "> ä¸¤è‚¡åŠ›é‡å…±åŒä½œç”¨ï¼š  \n",
    "> `total_loss = loss_data + Î» * loss_pde`  \n",
    "> å¼•å¯¼å‚æ•°èµ°å‘**æ—¢æ‹Ÿåˆæ•°æ®åˆæ»¡è¶³æ–¹ç¨‹**çš„æœ€ä¼˜è§£ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 ç‰©ç†å®šå¾‹ â†’ å¯ä¼˜åŒ–æŸå¤±å‡½æ•°\n",
    "\n",
    "| æ­¥éª¤ | æŠ€æœ¯ | è¯´æ˜ |\n",
    "|---|---|---|\n",
    "| **æ–¹ç¨‹** | $u_t + u u_x - \\nu u_{xx} = 0$ | Burgers æ–¹ç¨‹ |\n",
    "| **æ®‹å·®** | `residual = compute_pde_residual(x, t)` | åˆ©ç”¨ **PyTorch è‡ªåŠ¨å¾®åˆ†** è®¡ç®— |\n",
    "| **æŸå¤±** | `loss_pde = MSE(residual, 0)` | æŠŠç‰©ç†çº¦æŸå˜æˆ**å¯ç›´æ¥æœ€å°åŒ–çš„æ ‡é‡** |\n",
    "\n",
    "> PINN ç²¾é«“ï¼š  \n",
    "> **â€œè®©ç‰©ç†æ–¹ç¨‹æˆä¸ºç½‘ç»œçš„ä¸€å±‚æŸå¤±â€**ï¼Œè€Œä¸æ˜¯åéªŒæ ¡éªŒã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 å‚æ•°åæ¼” (Parameter Inference)\n",
    "\n",
    "| å¯å­¦ä¹ å¯¹è±¡ | ä»£ç  | ç»“æœ |\n",
    "|---|---|---|\n",
    "| **æœªçŸ¥ç‰©ç†å¸¸æ•° Î½** | `self.nu = nn.Parameter(...)` | ä¸ç½‘ç»œæƒé‡ **åŒæ­¥æ¥å—æ¢¯åº¦** |\n",
    "| **æ¢¯åº¦æ¥æº** | `total_loss.backward()` | PyTorch è‡ªåŠ¨è®¡ç®— âˆ‚loss/âˆ‚Î½ |\n",
    "| **æœ€ç»ˆæ•ˆæœ** | `optimizer.step()` | **åæ¨å‡ºæœ€åˆç†çš„ Î½**ï¼Œå®ç°**é€†é—®é¢˜æ±‚è§£** |\n",
    "\n",
    "> ğŸ” **ä¸€å¥è¯æ€»ç»“**ï¼š  \n",
    "> ç½‘ç»œä¸ä»…å­¦â€œè§£â€ï¼Œè¿˜å­¦â€œç‰©ç†å¸¸æ•°â€ï¼ŒçœŸæ­£åšåˆ° **â€œæ•°æ® + ç‰©ç†â€ åŒå‘è‡ªæ´½**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2098db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_dims):\n",
    "        super().__init__()\n",
    "        self.layyers = nn.ModuleList()\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            self.layyers.append(nn.Linear(layer_dims[i], layer_dims[i + 1]))\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layyers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.layyers[-1](x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dafea7b",
   "metadata": {},
   "source": [
    "MLP.forward çš„èŒè´£å¾ˆçº¯ç²¹ï¼šå°±æ˜¯æ‰§è¡Œä¸€ä¸ªæ ‡å‡†çš„å¤šå±‚æ„ŸçŸ¥æœºçš„å‰å‘è®¡ç®—ã€‚\n",
    "\n",
    "PINN_Burgers.forward çš„èŒè´£æ˜¯â€œç¼–æ’â€ï¼šå®ƒè´Ÿè´£å‡†å¤‡ MLP éœ€è¦çš„è¾“å…¥æ ¼å¼ï¼Œå¹¶è°ƒç”¨ MLP æ¥å®Œæˆè®¡ç®—ã€‚\n",
    "\n",
    "PINN_Burgers.compute_pde_residual çš„èŒè´£æ˜¯â€œç‰©ç†çº¦æŸâ€ï¼šå®ƒè°ƒç”¨ forward å¾—åˆ°è§£ uï¼Œç„¶ååˆ©ç”¨è‡ªåŠ¨å¾®åˆ†æ¥è®¡ç®—å’Œç‰©ç†æ–¹ç¨‹ç›¸å…³çš„æŸå¤±ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f8cc0",
   "metadata": {},
   "source": [
    "## 1. __init__ â€”â€” ç½‘ç»œåˆå§‹åŒ–\n",
    "\n",
    "\n",
    "| æ­¥éª¤          | å…³é”®åŠ¨ä½œ                             | è®¾è®¡æ„å›¾                           |\n",
    "| ----------- | -------------------------------- | ------------------------------ |\n",
    "| 1ï¸âƒ£ åˆå§‹åŒ–çˆ¶ç±»   | `super().__init__()`             | éµå¾ª PyTorch æ¨¡å—è§„èŒƒ                |\n",
    "| 2ï¸âƒ£ åˆ›å»ºæ ¸å¿ƒç½‘ç»œ  | `self.network = MLP(layer_dims)` | **æ¨¡å—åŒ–**ï¼šå¤ç”¨å·²å†™å¥½çš„ `MLP`ï¼Œé¿å…é‡å¤é€ è½®å­   |\n",
    "| 3ï¸âƒ£ å£°æ˜å¯å­¦ä¹ å‚æ•° | `self.nu = nn.Parameter(...)`    | **é€†é—®é¢˜æ ¸å¿ƒ**ï¼šè®© **é»åº¦ç³»æ•° Î½** ä¹Ÿå‚ä¸åå‘ä¼ æ’­ |\n",
    "\n",
    "ğŸ” äº®ç‚¹ï¼šæŠŠç‰©ç†å¸¸æ•° Î½ è®¾ä¸º nn.Parameterï¼Œç½‘ç»œä¸ä»…æ‹Ÿåˆè§£ u(x,t)ï¼Œè¿˜èƒ½ä»æ•°æ®é‡Œåæ¨æœ€åˆé€‚çš„ç‰©ç†å‚æ•°ã€‚\n",
    "\n",
    "\n",
    "\n",
    "## 2. forward â€”â€” æ•°æ®å‰å‘ä¼ æ’­\n",
    "\n",
    "| æ­¥éª¤ | å…³é”®åŠ¨ä½œ | è®¾è®¡æ„å›¾ |\n",
    "|---|---|---|\n",
    "| **1ï¸âƒ£ æ‹¼æ¥è¾“å…¥** | `torch.cat([x, t], dim=1)` | **æ—¶ç©ºæ•´åˆ**ï¼šæŠŠç©ºé—´åæ ‡ `x` å’Œæ—¶é—´åæ ‡ `t` ç»„åˆæˆ `[batch, 2]` çš„å•ä¸€è¾“å…¥ï¼Œè¿™æ˜¯ç¥ç»ç½‘ç»œå¤„ç†æ—¶ç©ºé—®é¢˜çš„å¸¸è§å¥—è·¯ã€‚ |\n",
    "| **2ï¸âƒ£ æ ¸å¿ƒè®¡ç®—** | `self.network(inputs)` | **åŠŸèƒ½è§£è€¦**ï¼šå°†çœŸæ­£çš„æ•°æ®æ˜ å°„é€»è¾‘å®Œå…¨äº¤ç»™å†…éƒ¨çš„ `MLP`ï¼Œä¸»ç½‘ç»œåªåšâ€œåŒ…è£…å™¨â€ï¼Œä¾¿äºåç»­æ›¿æ¢æˆ–å‡çº§ã€‚ |\n",
    "\n",
    "> ğŸ” **äº®ç‚¹**ï¼šæ•´ä¸ª `forward` åªæœ‰ä¸¤è¡Œä»£ç ï¼Œå´æ¸…æ™°å±•ç¤ºäº† **å¦‚ä½•é€šè¿‡æç®€çš„è¾“å…¥é¢„å¤„ç†ï¼ŒæŠŠä¸€ä¸ªé€šç”¨ MLP è¿…é€Ÿé€‚é…åˆ°æ—¶ç©ºé¢„æµ‹ä»»åŠ¡**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 3. compute_pde_residual â€”â€” ç‰©ç†ä¿¡æ¯æ ¸å¿ƒ\n",
    "\n",
    "| æ­¥éª¤ | å…³é”®åŠ¨ä½œ | è®¾è®¡æ„å›¾ |\n",
    "|---|---|---|\n",
    "| **1ï¸âƒ£ å¼€å¯å¾®åˆ†** | `x.requires_grad_(True)` | **å¯ç”¨è‡ªåŠ¨å¾®åˆ†**ï¼šå‘Šè¯‰ PyTorch éœ€è¦ä¸º `x`ã€`t` è®¡ç®—æ¢¯åº¦ï¼Œä¸ºåç»­æ±‚åå¯¼åšå‡†å¤‡ã€‚ |\n",
    "| **2ï¸âƒ£ è®¡ç®—åå¯¼æ•°** | `torch.autograd.grad(...)` | **æ ¸å¿ƒæœºåˆ¶**ï¼šåˆ©ç”¨ PyTorch çš„è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½ï¼Œè½»æ¾æ±‚å‡º `u_t`ã€`u_x`ã€`u_xx`ï¼Œè¿™æ˜¯ PINN çš„å…³é”®æŠ€æœ¯ã€‚ |\n",
    "| **3ï¸âƒ£ è®¡ç®—æ®‹å·®** | `residual = u_t + u * u_x - self.nu * u_xx` | **ç‰©ç†çº¦æŸ**ï¼šå°†åå¯¼æ•°ç›´æ¥ä»£å…¥ Burgers æ–¹ç¨‹ï¼Œå¾—åˆ°æ–¹ç¨‹çš„ä¸å¹³è¡¡é‡ï¼›è¯¥æ®‹å·®ä½œä¸ºæŸå¤±é¡¹ï¼Œå¼ºåˆ¶ç½‘ç»œéµå®ˆç‰©ç†å®šå¾‹ã€‚ |\n",
    "\n",
    "> ğŸ” **äº®ç‚¹**ï¼šè¯¥æ–¹æ³•æ˜¯æ•´ä¸ª PINN çš„ç²¾é«“â€”â€”**æŠŠæ•°å­¦ç‰©ç†æ–¹ç¨‹ç›´æ¥è½¬æˆç¥ç»ç½‘ç»œçš„å¯è®¡ç®—æŸå¤±**ï¼Œä½¿æ¨¡å‹åœ¨æ‹Ÿåˆæ•°æ®çš„åŒæ—¶ï¼Œä¹Ÿå­¦ä¼šéµå®ˆç‰©ç†è§„å¾‹ã€‚\n",
    "\n",
    "## 4. æ®‹å·®çš„ç‰©ç†æ„ä¹‰ä¸è®­ç»ƒä½œç”¨\n",
    "\n",
    "| æ¦‚å¿µ | å…¬å¼ / ä»£ç  | ä½œç”¨ |\n",
    "|---|---|---|\n",
    "| **Burgers æ–¹ç¨‹** | $u_t + u u_x - \\nu u_{xx} = 0$ | æè¿°ä¸€ç»´ç²˜æ€§æµä½“è¿åŠ¨çš„ç‰©ç†è§„å¾‹ |\n",
    "| **æ®‹å·®ï¼ˆresidualï¼‰** | `residual = u_t + u * u_x - self.nu * u_xx` | ç½‘ç»œé¢„æµ‹ä»£å…¥æ–¹ç¨‹å **å·¦ä¾§ â‰  0** çš„å‰©ä½™é‡ |\n",
    "| **è®­ç»ƒç›®æ ‡** | $\\mathcal{L}_{\\text{pde}} = \\text{MSE}(\\text{residual},\\ 0)$ | å°†æ®‹å·®ä½œä¸ºé¢å¤–æŸå¤±é¡¹ï¼Œ**é€¼è¿«ç½‘ç»œè®©æ®‹å·® â†’ 0** |\n",
    "| **æœ€ç»ˆæ•ˆæœ** | â€”â€” | æ¨¡å‹åœ¨**æ‹Ÿåˆè§‚æµ‹æ•°æ®**çš„åŒæ—¶ï¼Œä¹Ÿ**ä¸¥æ ¼éµå®ˆç‰©ç†æ–¹ç¨‹** |\n",
    "\n",
    "> ğŸ” **ä¸€å¥è¯æ€»ç»“**ï¼šæ®‹å·®æŠŠâ€œç‰©ç†å®šå¾‹â€ç¿»è¯‘æˆå¯å¾®çš„æŸå¤±å‡½æ•°ï¼Œè®©ä¼˜åŒ–å™¨åœ¨æœ€å°åŒ–æ•°æ®è¯¯å·®çš„åŒæ—¶ï¼Œä¹Ÿæœ€å°åŒ–â€œè¿åç‰©ç†â€çš„ç¨‹åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ddb3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_Burgers(nn.Module):\n",
    "    def __init__(self, layer_dims, true_nu=None):\n",
    "        super().__init__()\n",
    "        self.network = MLP(layer_dims)\n",
    "        \n",
    "        # å°† nu å®šä¹‰ä¸ºä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°\n",
    "        # æˆ‘ä»¬ç”¨ä¸€ä¸ªçŒœæµ‹å€¼è¿›è¡Œåˆå§‹åŒ–ï¼Œä¾‹å¦‚ 0.1\n",
    "        # å¦‚æœæä¾›äº† true_nuï¼Œåˆ™ä½¿ç”¨å®ƒï¼ˆç”¨äºè°ƒè¯•/æµ‹è¯•æ­£å‘é—®é¢˜ï¼‰\n",
    "        initial_nu = 0.1 if true_nu is None else true_nu\n",
    "        self.nu = nn.Parameter(torch.tensor([initial_nu], dtype=torch.float32, requires_grad=True))\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        # æ‹¼æ¥ x å’Œ t ä»¥åˆ›å»ºç½‘ç»œè¾“å…¥\n",
    "        inputs = torch.cat([x, t], dim=1)\n",
    "        return self.network(inputs)\n",
    "    \n",
    "    def compute_pde_residual(self, x, t):\n",
    "        # ä¸ºè¾“å…¥è®¾ç½® requires_grad=True ä»¥è®¡ç®—å¯¼æ•°\n",
    "        x.requires_grad_(True)\n",
    "        t.requires_grad_(True)\n",
    "        \n",
    "        u = self.forward(x, t)\n",
    "        \n",
    "        # ä½¿ç”¨è‡ªåŠ¨å¾®åˆ†è®¡ç®—å¯¼æ•°\n",
    "        u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "        \n",
    "        # ä¼¯æ ¼æ–¯æ–¹ç¨‹æ®‹å·®\n",
    "        residual = u_t + u * u_x - self.nu * u_xx\n",
    "        return residual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55e82c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# -- 2. ä¸»è®­ç»ƒè„šæœ¬ --\n",
    "if __name__ == '__main__':\n",
    "    # è®¾å¤‡è®¾ç½®\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    " # --- æ•°æ®åŠ è½½å’Œå‡†å¤‡ ---\n",
    "    data = np.load('burgers_shock_solution.npz')\n",
    "    x_data = torch.tensor(data['x'], dtype=torch.float32) # å½¢çŠ¶: [nx, 1]\n",
    "    t_data = torch.tensor(data['t'], dtype=torch.float32) # å½¢çŠ¶: [nt, 1]\n",
    "    u_solution = torch.tensor(data['u'], dtype=torch.float32) # å½¢çŠ¶: [nt, nx]\n",
    "\n",
    "    # åˆ›å»ºç”¨äºè®­ç»ƒçš„åæ ‡ç½‘æ ¼\n",
    "    # æˆ‘ä»¬å°†åœ¨æ‰€æœ‰æ•°æ®ç‚¹ä¸Šè¿›è¡Œè®­ç»ƒ\n",
    "    T, X = torch.meshgrid(t_data.squeeze(), x_data.squeeze(), indexing='ij')\n",
    "    \n",
    "   # å‡†å¤‡è®­ç»ƒæ•°æ®ä¸‰å…ƒç»„ (x, t, u)\n",
    "    x_train = X.reshape(-1, 1)\n",
    "    t_train = T.reshape(-1, 1)\n",
    "    u_train = u_solution.reshape(-1, 1)\n",
    "\n",
    "# å°†æ‰€æœ‰è®­ç»ƒæ•°æ®ç§»åŠ¨åˆ°é€‰å®šçš„è®¾å¤‡\n",
    "    x_train = x_train.to(device)\n",
    "    t_train = t_train.to(device)\n",
    "    u_train = u_train.to(device)\n",
    "\n",
    "\n",
    "  # --- ä½¿ç”¨ TensorDataset å’Œ DataLoader åˆ›å»ºå°æ‰¹é‡æ•°æ® ---\n",
    "    # å®šä¹‰æ‰¹æ¬¡å¤§å°ï¼Œæ‚¨å¯ä»¥æ ¹æ®æ‚¨çš„GPUæ˜¾å­˜è¿›è¡Œè°ƒæ•´\n",
    "    # batch_size = 2048\n",
    "    # train_dataset = TensorDataset(x_train, t_train, u_train)\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# --- æ¨¡å‹ã€æŸå¤±å’Œä¼˜åŒ–å™¨è®¾ç½® ---\n",
    "# å¯¹äºé€†å‘é—®é¢˜ï¼Œæˆ‘ä»¬ä¸æä¾›çœŸå®çš„ nu\n",
    "# pinn_model.parameters() æ˜¯ä¸€ä¸ªâ€œå‚æ•°æ”¶é›†å™¨,è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ‰¾åˆ°çš„å‚æ•°çš„é›†åˆ\n",
    "pinn_model = PINN_Burgers(layer_dims=[2, 20, 1]).to(device)\n",
    "optimizer = torch.optim.Adam(pinn_model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebb66e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\27346\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:181.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/20000], Total Loss: 0.1132, Data Loss: 0.1025, Physics Loss: 0.0107, Predicted nu: 0.3449\n",
      "Epoch [2000/20000], Total Loss: 0.0647, Data Loss: 0.0564, Physics Loss: 0.0083, Predicted nu: 0.3075\n",
      "Epoch [3000/20000], Total Loss: 0.0468, Data Loss: 0.0396, Physics Loss: 0.0072, Predicted nu: 0.2517\n",
      "Epoch [4000/20000], Total Loss: 0.0372, Data Loss: 0.0314, Physics Loss: 0.0058, Predicted nu: 0.2128\n",
      "Epoch [5000/20000], Total Loss: 0.0322, Data Loss: 0.0267, Physics Loss: 0.0054, Predicted nu: 0.1936\n",
      "Epoch [6000/20000], Total Loss: 0.0297, Data Loss: 0.0241, Physics Loss: 0.0055, Predicted nu: 0.1764\n",
      "Epoch [7000/20000], Total Loss: 0.0281, Data Loss: 0.0225, Physics Loss: 0.0056, Predicted nu: 0.1625\n",
      "Epoch [8000/20000], Total Loss: 0.0269, Data Loss: 0.0213, Physics Loss: 0.0056, Predicted nu: 0.1506\n",
      "Epoch [9000/20000], Total Loss: 0.0260, Data Loss: 0.0203, Physics Loss: 0.0057, Predicted nu: 0.1393\n",
      "Epoch [10000/20000], Total Loss: 0.0238, Data Loss: 0.0189, Physics Loss: 0.0050, Predicted nu: 0.1184\n",
      "Epoch [11000/20000], Total Loss: 0.0215, Data Loss: 0.0168, Physics Loss: 0.0046, Predicted nu: 0.1050\n",
      "Epoch [12000/20000], Total Loss: 0.0208, Data Loss: 0.0162, Physics Loss: 0.0046, Predicted nu: 0.1010\n",
      "Epoch [13000/20000], Total Loss: 0.0205, Data Loss: 0.0158, Physics Loss: 0.0046, Predicted nu: 0.0988\n",
      "Epoch [14000/20000], Total Loss: 0.0202, Data Loss: 0.0155, Physics Loss: 0.0046, Predicted nu: 0.0972\n",
      "Epoch [15000/20000], Total Loss: 0.0199, Data Loss: 0.0153, Physics Loss: 0.0046, Predicted nu: 0.0958\n",
      "Epoch [16000/20000], Total Loss: 0.0196, Data Loss: 0.0151, Physics Loss: 0.0045, Predicted nu: 0.0945\n",
      "Epoch [17000/20000], Total Loss: 0.0194, Data Loss: 0.0149, Physics Loss: 0.0044, Predicted nu: 0.0932\n",
      "Epoch [18000/20000], Total Loss: 0.0192, Data Loss: 0.0147, Physics Loss: 0.0045, Predicted nu: 0.0919\n",
      "Epoch [19000/20000], Total Loss: 0.0188, Data Loss: 0.0145, Physics Loss: 0.0042, Predicted nu: 0.0904\n",
      "Epoch [20000/20000], Total Loss: 0.0184, Data Loss: 0.0143, Physics Loss: 0.0041, Predicted nu: 0.0889\n",
      "\n",
      "Training finished!\n",
      "The final predicted viscosity nu is: 0.08890\n",
      "(The true value was 0.07)\n"
     ]
    }
   ],
   "source": [
    " # --- è®­ç»ƒå¾ªç¯ ---\n",
    "epochs = 20000\n",
    "# å®šä¹‰ä¸€ä¸ªè£å‰ªé˜ˆå€¼\n",
    "clip_value = 1.0 \n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 1. æ•°æ®æŸå¤±\n",
    "    u_pred = pinn_model(x_train, t_train)\n",
    "    data_loss = loss_fn(u_pred, u_train)\n",
    "# 2. ç‰©ç†æŸå¤±\n",
    "    # å¯¹äºç‰©ç†æŸå¤±ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç›¸åŒçš„ç‚¹æˆ–é‡‡æ ·æ–°çš„ç‚¹ã€‚\n",
    "    # è¿™é‡Œä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„è®­ç»ƒç‚¹ã€‚\n",
    "    pde_residual = pinn_model.compute_pde_residual(x_train, t_train)\n",
    "    physics_loss = loss_fn(pde_residual, torch.zeros_like(pde_residual))\n",
    "\n",
    "    # æˆ–è€… 0.01ï¼Œè¿™æ˜¯ä¸€ä¸ªéœ€è¦è°ƒæ•´çš„è¶…å‚æ•°\n",
    "    lambda_physics = 10\n",
    "    physics_loss = lambda_physics * physics_loss\n",
    "    # æ€»æŸå¤±ï¼ˆæ‚¨å¯ä»¥æ·»åŠ ä¸€ä¸ªæƒé‡ï¼Œä¾‹å¦‚ total_loss = data_loss + 0.1 * physics_lossï¼‰\n",
    "    total_loss = data_loss + physics_loss\n",
    "\n",
    "    total_loss.backward()\n",
    "\n",
    "    # åœ¨æ›´æ–°ä¹‹å‰è¿›è¡Œæ¢¯åº¦è£å‰ª\n",
    "    torch.nn.utils.clip_grad_norm_(pinn_model.parameters(), clip_value)\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        # ç§˜å¯†ç­”æ¡ˆæ˜¯ 0.07ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„çŒœæµ‹æœ‰å¤šæ¥è¿‘ï¼\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Total Loss: {total_loss.item():.4f}, \"\n",
    "                f\"Data Loss: {data_loss.item():.4f}, Physics Loss: {physics_loss.item():.4f}, \"\n",
    "                f\"Predicted nu: {pinn_model.nu.item():.4f}\")\n",
    "print(\"\\nTraining finished!\")\n",
    "print(f\"The final predicted viscosity nu is: {pinn_model.nu.item():.5f}\")\n",
    "print(f\"(The true value was 0.07)\") \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ebb5053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ ‡å‡† PINN æ¨¡å‹\n",
    "PATH = r\"D:\\A-Code\\GitHubCode\\Pytorch-PINN-Learning\\model\\stage3_ipinn_model.pth\"\n",
    "torch.save(pinn_model.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Python 3.13)",
   "language": "python",
   "name": "my_python313_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
