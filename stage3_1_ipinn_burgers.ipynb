{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c30d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f7747a",
   "metadata": {},
   "source": [
    "## 5. 核心模式 (Core Pattern)\n",
    "\n",
    "### 5.1 混合驱动的优化 (Hybrid-Driven Optimization)\n",
    "\n",
    "| 驱动力 | 角色 | 作用方式 |\n",
    "|---|---|---|\n",
    "| **Data-Driven** | 已知的稀疏观测点 | 通过 `loss_data = loss_fn(u_pred, u_label)` 保证模型**拟合观测数据** |\n",
    "| **Physics-Informed** | 无处不在的 PDE | 通过 `loss_pde = loss_fn(residual, 0)` 强制模型**遵守物理定律** |\n",
    "\n",
    "> 两股力量共同作用：  \n",
    "> `total_loss = loss_data + λ * loss_pde`  \n",
    "> 引导参数走向**既拟合数据又满足方程**的最优解。\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 物理定律 → 可优化损失函数\n",
    "\n",
    "| 步骤 | 技术 | 说明 |\n",
    "|---|---|---|\n",
    "| **方程** | $u_t + u u_x - \\nu u_{xx} = 0$ | Burgers 方程 |\n",
    "| **残差** | `residual = compute_pde_residual(x, t)` | 利用 **PyTorch 自动微分** 计算 |\n",
    "| **损失** | `loss_pde = MSE(residual, 0)` | 把物理约束变成**可直接最小化的标量** |\n",
    "\n",
    "> PINN 精髓：  \n",
    "> **“让物理方程成为网络的一层损失”**，而不是后验校验。\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 参数反演 (Parameter Inference)\n",
    "\n",
    "| 可学习对象 | 代码 | 结果 |\n",
    "|---|---|---|\n",
    "| **未知物理常数 ν** | `self.nu = nn.Parameter(...)` | 与网络权重 **同步接受梯度** |\n",
    "| **梯度来源** | `total_loss.backward()` | PyTorch 自动计算 ∂loss/∂ν |\n",
    "| **最终效果** | `optimizer.step()` | **反推出最合理的 ν**，实现**逆问题求解** |\n",
    "\n",
    "> 🔍 **一句话总结**：  \n",
    "> 网络不仅学“解”，还学“物理常数”，真正做到 **“数据 + 物理” 双向自洽**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2098db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_dims):\n",
    "        super().__init__()\n",
    "        self.layyers = nn.ModuleList()\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            self.layyers.append(nn.Linear(layer_dims[i], layer_dims[i + 1]))\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layyers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.layyers[-1](x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dafea7b",
   "metadata": {},
   "source": [
    "MLP.forward 的职责很纯粹：就是执行一个标准的多层感知机的前向计算。\n",
    "\n",
    "PINN_Burgers.forward 的职责是“编排”：它负责准备 MLP 需要的输入格式，并调用 MLP 来完成计算。\n",
    "\n",
    "PINN_Burgers.compute_pde_residual 的职责是“物理约束”：它调用 forward 得到解 u，然后利用自动微分来计算和物理方程相关的损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f8cc0",
   "metadata": {},
   "source": [
    "## 1. __init__ —— 网络初始化\n",
    "\n",
    "\n",
    "| 步骤          | 关键动作                             | 设计意图                           |\n",
    "| ----------- | -------------------------------- | ------------------------------ |\n",
    "| 1️⃣ 初始化父类   | `super().__init__()`             | 遵循 PyTorch 模块规范                |\n",
    "| 2️⃣ 创建核心网络  | `self.network = MLP(layer_dims)` | **模块化**：复用已写好的 `MLP`，避免重复造轮子   |\n",
    "| 3️⃣ 声明可学习参数 | `self.nu = nn.Parameter(...)`    | **逆问题核心**：让 **黏度系数 ν** 也参与反向传播 |\n",
    "\n",
    "🔍 亮点：把物理常数 ν 设为 nn.Parameter，网络不仅拟合解 u(x,t)，还能从数据里反推最合适的物理参数。\n",
    "\n",
    "\n",
    "\n",
    "## 2. forward —— 数据前向传播\n",
    "\n",
    "| 步骤 | 关键动作 | 设计意图 |\n",
    "|---|---|---|\n",
    "| **1️⃣ 拼接输入** | `torch.cat([x, t], dim=1)` | **时空整合**：把空间坐标 `x` 和时间坐标 `t` 组合成 `[batch, 2]` 的单一输入，这是神经网络处理时空问题的常见套路。 |\n",
    "| **2️⃣ 核心计算** | `self.network(inputs)` | **功能解耦**：将真正的数据映射逻辑完全交给内部的 `MLP`，主网络只做“包装器”，便于后续替换或升级。 |\n",
    "\n",
    "> 🔍 **亮点**：整个 `forward` 只有两行代码，却清晰展示了 **如何通过极简的输入预处理，把一个通用 MLP 迅速适配到时空预测任务**。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. compute_pde_residual —— 物理信息核心\n",
    "\n",
    "| 步骤 | 关键动作 | 设计意图 |\n",
    "|---|---|---|\n",
    "| **1️⃣ 开启微分** | `x.requires_grad_(True)` | **启用自动微分**：告诉 PyTorch 需要为 `x`、`t` 计算梯度，为后续求偏导做准备。 |\n",
    "| **2️⃣ 计算偏导数** | `torch.autograd.grad(...)` | **核心机制**：利用 PyTorch 的自动微分功能，轻松求出 `u_t`、`u_x`、`u_xx`，这是 PINN 的关键技术。 |\n",
    "| **3️⃣ 计算残差** | `residual = u_t + u * u_x - self.nu * u_xx` | **物理约束**：将偏导数直接代入 Burgers 方程，得到方程的不平衡量；该残差作为损失项，强制网络遵守物理定律。 |\n",
    "\n",
    "> 🔍 **亮点**：该方法是整个 PINN 的精髓——**把数学物理方程直接转成神经网络的可计算损失**，使模型在拟合数据的同时，也学会遵守物理规律。\n",
    "\n",
    "## 4. 残差的物理意义与训练作用\n",
    "\n",
    "| 概念 | 公式 / 代码 | 作用 |\n",
    "|---|---|---|\n",
    "| **Burgers 方程** | $u_t + u u_x - \\nu u_{xx} = 0$ | 描述一维粘性流体运动的物理规律 |\n",
    "| **残差（residual）** | `residual = u_t + u * u_x - self.nu * u_xx` | 网络预测代入方程后 **左侧 ≠ 0** 的剩余量 |\n",
    "| **训练目标** | $\\mathcal{L}_{\\text{pde}} = \\text{MSE}(\\text{residual},\\ 0)$ | 将残差作为额外损失项，**逼迫网络让残差 → 0** |\n",
    "| **最终效果** | —— | 模型在**拟合观测数据**的同时，也**严格遵守物理方程** |\n",
    "\n",
    "> 🔍 **一句话总结**：残差把“物理定律”翻译成可微的损失函数，让优化器在最小化数据误差的同时，也最小化“违反物理”的程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ddb3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_Burgers(nn.Module):\n",
    "    def __init__(self, layer_dims, true_nu=None):\n",
    "        super().__init__()\n",
    "        self.network = MLP(layer_dims)\n",
    "        \n",
    "        # 将 nu 定义为一个可学习的参数\n",
    "        # 我们用一个猜测值进行初始化，例如 0.1\n",
    "        # 如果提供了 true_nu，则使用它（用于调试/测试正向问题）\n",
    "        initial_nu = 0.1 if true_nu is None else true_nu\n",
    "        self.nu = nn.Parameter(torch.tensor([initial_nu], dtype=torch.float32, requires_grad=True))\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        # 拼接 x 和 t 以创建网络输入\n",
    "        inputs = torch.cat([x, t], dim=1)\n",
    "        return self.network(inputs)\n",
    "    \n",
    "    def compute_pde_residual(self, x, t):\n",
    "        # 为输入设置 requires_grad=True 以计算导数\n",
    "        x.requires_grad_(True)\n",
    "        t.requires_grad_(True)\n",
    "        \n",
    "        u = self.forward(x, t)\n",
    "        \n",
    "        # 使用自动微分计算导数\n",
    "        u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "        \n",
    "        # 伯格斯方程残差\n",
    "        residual = u_t + u * u_x - self.nu * u_xx\n",
    "        return residual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55e82c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# -- 2. 主训练脚本 --\n",
    "if __name__ == '__main__':\n",
    "    # 设备设置\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    " # --- 数据加载和准备 ---\n",
    "    data = np.load('burgers_shock_solution.npz')\n",
    "    x_data = torch.tensor(data['x'], dtype=torch.float32) # 形状: [nx, 1]\n",
    "    t_data = torch.tensor(data['t'], dtype=torch.float32) # 形状: [nt, 1]\n",
    "    u_solution = torch.tensor(data['u'], dtype=torch.float32) # 形状: [nt, nx]\n",
    "\n",
    "    # 创建用于训练的坐标网格\n",
    "    # 我们将在所有数据点上进行训练\n",
    "    T, X = torch.meshgrid(t_data.squeeze(), x_data.squeeze(), indexing='ij')\n",
    "    \n",
    "   # 准备训练数据三元组 (x, t, u)\n",
    "    x_train = X.reshape(-1, 1)\n",
    "    t_train = T.reshape(-1, 1)\n",
    "    u_train = u_solution.reshape(-1, 1)\n",
    "\n",
    "# 将所有训练数据移动到选定的设备\n",
    "    x_train = x_train.to(device)\n",
    "    t_train = t_train.to(device)\n",
    "    u_train = u_train.to(device)\n",
    "\n",
    "\n",
    "  # --- 使用 TensorDataset 和 DataLoader 创建小批量数据 ---\n",
    "    # 定义批次大小，您可以根据您的GPU显存进行调整\n",
    "    # batch_size = 2048\n",
    "    # train_dataset = TensorDataset(x_train, t_train, u_train)\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# --- 模型、损失和优化器设置 ---\n",
    "# 对于逆向问题，我们不提供真实的 nu\n",
    "# pinn_model.parameters() 是一个“参数收集器,返回一个包含所有找到的参数的集合\n",
    "pinn_model = PINN_Burgers(layer_dims=[2, 20, 1]).to(device)\n",
    "optimizer = torch.optim.Adam(pinn_model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebb66e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\27346\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:181.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/20000], Total Loss: 0.1132, Data Loss: 0.1025, Physics Loss: 0.0107, Predicted nu: 0.3449\n",
      "Epoch [2000/20000], Total Loss: 0.0647, Data Loss: 0.0564, Physics Loss: 0.0083, Predicted nu: 0.3075\n",
      "Epoch [3000/20000], Total Loss: 0.0468, Data Loss: 0.0396, Physics Loss: 0.0072, Predicted nu: 0.2517\n",
      "Epoch [4000/20000], Total Loss: 0.0372, Data Loss: 0.0314, Physics Loss: 0.0058, Predicted nu: 0.2128\n",
      "Epoch [5000/20000], Total Loss: 0.0322, Data Loss: 0.0267, Physics Loss: 0.0054, Predicted nu: 0.1936\n",
      "Epoch [6000/20000], Total Loss: 0.0297, Data Loss: 0.0241, Physics Loss: 0.0055, Predicted nu: 0.1764\n",
      "Epoch [7000/20000], Total Loss: 0.0281, Data Loss: 0.0225, Physics Loss: 0.0056, Predicted nu: 0.1625\n",
      "Epoch [8000/20000], Total Loss: 0.0269, Data Loss: 0.0213, Physics Loss: 0.0056, Predicted nu: 0.1506\n",
      "Epoch [9000/20000], Total Loss: 0.0260, Data Loss: 0.0203, Physics Loss: 0.0057, Predicted nu: 0.1393\n",
      "Epoch [10000/20000], Total Loss: 0.0238, Data Loss: 0.0189, Physics Loss: 0.0050, Predicted nu: 0.1184\n",
      "Epoch [11000/20000], Total Loss: 0.0215, Data Loss: 0.0168, Physics Loss: 0.0046, Predicted nu: 0.1050\n",
      "Epoch [12000/20000], Total Loss: 0.0208, Data Loss: 0.0162, Physics Loss: 0.0046, Predicted nu: 0.1010\n",
      "Epoch [13000/20000], Total Loss: 0.0205, Data Loss: 0.0158, Physics Loss: 0.0046, Predicted nu: 0.0988\n",
      "Epoch [14000/20000], Total Loss: 0.0202, Data Loss: 0.0155, Physics Loss: 0.0046, Predicted nu: 0.0972\n",
      "Epoch [15000/20000], Total Loss: 0.0199, Data Loss: 0.0153, Physics Loss: 0.0046, Predicted nu: 0.0958\n",
      "Epoch [16000/20000], Total Loss: 0.0196, Data Loss: 0.0151, Physics Loss: 0.0045, Predicted nu: 0.0945\n",
      "Epoch [17000/20000], Total Loss: 0.0194, Data Loss: 0.0149, Physics Loss: 0.0044, Predicted nu: 0.0932\n",
      "Epoch [18000/20000], Total Loss: 0.0192, Data Loss: 0.0147, Physics Loss: 0.0045, Predicted nu: 0.0919\n",
      "Epoch [19000/20000], Total Loss: 0.0188, Data Loss: 0.0145, Physics Loss: 0.0042, Predicted nu: 0.0904\n",
      "Epoch [20000/20000], Total Loss: 0.0184, Data Loss: 0.0143, Physics Loss: 0.0041, Predicted nu: 0.0889\n",
      "\n",
      "Training finished!\n",
      "The final predicted viscosity nu is: 0.08890\n",
      "(The true value was 0.07)\n"
     ]
    }
   ],
   "source": [
    " # --- 训练循环 ---\n",
    "epochs = 20000\n",
    "# 定义一个裁剪阈值\n",
    "clip_value = 1.0 \n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 1. 数据损失\n",
    "    u_pred = pinn_model(x_train, t_train)\n",
    "    data_loss = loss_fn(u_pred, u_train)\n",
    "# 2. 物理损失\n",
    "    # 对于物理损失，我们可以使用相同的点或采样新的点。\n",
    "    # 这里为了简单起见，我们使用相同的训练点。\n",
    "    pde_residual = pinn_model.compute_pde_residual(x_train, t_train)\n",
    "    physics_loss = loss_fn(pde_residual, torch.zeros_like(pde_residual))\n",
    "\n",
    "    # 或者 0.01，这是一个需要调整的超参数\n",
    "    lambda_physics = 10\n",
    "    physics_loss = lambda_physics * physics_loss\n",
    "    # 总损失（您可以添加一个权重，例如 total_loss = data_loss + 0.1 * physics_loss）\n",
    "    total_loss = data_loss + physics_loss\n",
    "\n",
    "    total_loss.backward()\n",
    "\n",
    "    # 在更新之前进行梯度裁剪\n",
    "    torch.nn.utils.clip_grad_norm_(pinn_model.parameters(), clip_value)\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        # 秘密答案是 0.07。让我们看看我们的猜测有多接近！\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Total Loss: {total_loss.item():.4f}, \"\n",
    "                f\"Data Loss: {data_loss.item():.4f}, Physics Loss: {physics_loss.item():.4f}, \"\n",
    "                f\"Predicted nu: {pinn_model.nu.item():.4f}\")\n",
    "print(\"\\nTraining finished!\")\n",
    "print(f\"The final predicted viscosity nu is: {pinn_model.nu.item():.5f}\")\n",
    "print(f\"(The true value was 0.07)\") \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ebb5053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存标准 PINN 模型\n",
    "PATH = r\"D:\\A-Code\\GitHubCode\\Pytorch-PINN-Learning\\model\\stage3_ipinn_model.pth\"\n",
    "torch.save(pinn_model.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Python 3.13)",
   "language": "python",
   "name": "my_python313_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
