{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4265a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -- 1. MLP和PINN模型定义 --\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    一个标准的多层感知机（MLP）\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dims):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # 使用 Xavier/Glorot 初始化来帮助稳定训练\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            linear_layer = nn.Linear(layer_dims[i], layer_dims[i+1])\n",
    "            nn.init.xavier_uniform_(linear_layer.weight)\n",
    "            nn.init.zeros_(linear_layer.bias)\n",
    "            self.layers.append(linear_layer)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = self.activation(self.layers[i](x))\n",
    "        # 最后一层没有激活函数\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "class PINN_Burgers(nn.Module):\n",
    "    \"\"\"\n",
    "    用于伯格斯方程的物理信息神经网络\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dims, true_nu=None):\n",
    "        super().__init__()\n",
    "        self.network = MLP(layer_dims)\n",
    "        \n",
    "        # 将 nu 定义为一个可学习的参数\n",
    "        initial_nu = 0.1 if true_nu is None else true_nu\n",
    "        # 将 nu 的值限制为正数，使用 softplus 的逆变换来初始化\n",
    "        # 这样可以保证在优化过程中 nu 始终为正\n",
    "        self.log_nu = nn.Parameter(torch.tensor([np.log(initial_nu)], dtype=torch.float32))\n",
    "\n",
    "    @property\n",
    "    def nu(self):\n",
    "        # 使用 softplus 来确保 nu 始终为正\n",
    "        # 这比直接优化 nu 更稳定\n",
    "        return torch.exp(self.log_nu)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # 拼接 x 和 t 以创建网络输入\n",
    "        inputs = torch.cat([x, t], dim=1)\n",
    "        return self.network(inputs)\n",
    "\n",
    "    def compute_pde_residual(self, x, t):\n",
    "        # 为输入设置 requires_grad=True 以计算导数\n",
    "        x.requires_grad_(True)\n",
    "        t.requires_grad_(True)\n",
    "        \n",
    "        u = self.forward(x, t)\n",
    "        \n",
    "        # 使用自动微分计算导数\n",
    "        # create_graph=True 允许我们计算高阶导数\n",
    "        u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "        \n",
    "        # 伯格斯方程残差: u_t + u * u_x - nu * u_xx = 0\n",
    "        residual = u_t + u * u_x - self.nu * u_xx\n",
    "        return residual\n",
    "\n",
    "# -- 2. 主训练脚本 --\n",
    "if __name__ == '__main__':\n",
    "    # 设备设置\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- 数据加载和准备 ---\n",
    "    try:\n",
    "        data = np.load('burgers_shock_solution.npz')\n",
    "        x_data = torch.tensor(data['x'], dtype=torch.float32) \n",
    "        t_data = torch.tensor(data['t'], dtype=torch.float32)\n",
    "        u_solution = torch.tensor(data['u'], dtype=torch.float32)\n",
    "    except FileNotFoundError:\n",
    "        print(\"错误：找不到 'burgers_shock_solution.npz' 文件。\")\n",
    "        print(\"请确保该文件与脚本在同一目录下。\")\n",
    "        exit()\n",
    "\n",
    "    # 创建用于训练的坐标网格\n",
    "    T, X = torch.meshgrid(t_data.squeeze(), x_data.squeeze(), indexing='ij')\n",
    "    \n",
    "    # 准备训练数据三元组 (x, t, u)\n",
    "    x_train = X.reshape(-1, 1)\n",
    "    t_train = T.reshape(-1, 1)\n",
    "    u_train = u_solution.reshape(-1, 1)\n",
    "    \n",
    "    # 将所有训练数据移动到选定的设备\n",
    "    x_train = x_train.to(device)\n",
    "    t_train = t_train.to(device)\n",
    "    u_train = u_train.to(device)\n",
    "\n",
    "    # --- 模型、损失和优化器设置 ---\n",
    "    pinn_model = PINN_Burgers(layer_dims=[2, 20, 20, 20, 20, 1]).to(device)\n",
    "    \n",
    "    # 降低学习率以增加稳定性\n",
    "    optimizer = torch.optim.Adam(pinn_model.parameters(), lr=1e-4)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # --- 训练循环 ---\n",
    "    epochs = 20000\n",
    "    # 物理损失的权重，这是一个需要仔细调整的关键超参数\n",
    "    lambda_physics = 1e-2 \n",
    "    # 梯度裁剪的阈值\n",
    "    clip_value = 1.0\n",
    "\n",
    "    print(\"--- 开始训练 ---\")\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 1. 数据损失\n",
    "        u_pred = pinn_model(x_train, t_train)\n",
    "        data_loss = loss_fn(u_pred, u_train)\n",
    "        \n",
    "        # 2. 物理损失\n",
    "        pde_residual = pinn_model.compute_pde_residual(x_train, t_train)\n",
    "        physics_loss_raw = loss_fn(pde_residual, torch.zeros_like(pde_residual))\n",
    "        \n",
    "        # 组合损失\n",
    "        total_loss = data_loss + lambda_physics * physics_loss_raw\n",
    "        \n",
    "        # 检查损失是否为 nan\n",
    "        if torch.isnan(total_loss):\n",
    "            print(f\"错误：在第 {epoch+1} 个 epoch 损失变为 NaN。训练提前终止。\")\n",
    "            print(\"建议尝试进一步降低学习率或 lambda_physics 的值。\")\n",
    "            break\n",
    "            \n",
    "        total_loss.backward()\n",
    "        \n",
    "        # 在更新之前进行梯度裁剪，防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(pinn_model.parameters(), clip_value)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            # 打印未加权的物理损失，以便更好地监控其量级\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Total Loss: {total_loss.item():.6f}, \"\n",
    "                  f\"Data Loss: {data_loss.item():.6f}, Physics Loss (raw): {physics_loss_raw.item():.6f}, \"\n",
    "                  f\"Predicted nu: {pinn_model.nu.item():.5f}\")\n",
    "\n",
    "    print(\"\\n--- 训练完成 ---\")\n",
    "    if not torch.isnan(total_loss):\n",
    "        print(f\"最终预测的粘性系数 nu 是: {pinn_model.nu.item():.5f}\")\n",
    "        print(f\"（真实值为 0.07）\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
