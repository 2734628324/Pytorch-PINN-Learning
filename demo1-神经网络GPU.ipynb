{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "480d9bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GPU (CUDA) 检查 ---\n",
      "CUDA 可用！正在使用 GPU: NVIDIA GeForce GTX 1650 Ti\n",
      "GPU 数量: 1\n",
      "------------------------------\n",
      "\n",
      "--- 开始训练神经网络 ---\n",
      "Epoch [10/100], Loss: 1.0540\n",
      "Epoch [20/100], Loss: 1.0379\n",
      "Epoch [30/100], Loss: 1.0276\n",
      "Epoch [40/100], Loss: 1.0196\n",
      "Epoch [50/100], Loss: 1.0127\n",
      "Epoch [60/100], Loss: 1.0064\n",
      "Epoch [70/100], Loss: 1.0005\n",
      "Epoch [80/100], Loss: 0.9949\n",
      "Epoch [90/100], Loss: 0.9896\n",
      "Epoch [100/100], Loss: 0.9845\n",
      "--- 训练完成 ---\n",
      "\n",
      "--- 模型评估 (在训练数据上) ---\n",
      "评估损失 (MSE): 0.9840\n",
      "\n",
      "--- 学到的模型参数示例 (第一层权重和偏置) ---\n",
      "第一层权重形状: torch.Size([20, 10])\n",
      "第一层权重示例:\n",
      "tensor([[-0.0308, -0.0585,  0.2447, -0.0489, -0.1256],\n",
      "        [ 0.2941, -0.3007, -0.0580,  0.0299,  0.1804]], device='cuda:0')\n",
      "第一层偏置示例:\n",
      "tensor([-0.0338,  0.0289,  0.1695,  0.2469, -0.1465], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# --- 1. 检查 GPU (CUDA) 是否可用并打印 GPU 型号 ---\n",
    "print(\"--- GPU (CUDA) 检查 ---\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA 可用！正在使用 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 数量: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA 不可用，正在使用 CPU 进行训练。\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 2. 定义一个简单的神经网络 ---\n",
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# --- 3. 设置模型参数和数据 ---\n",
    "input_size = 10    # 输入特征的数量\n",
    "hidden_size = 20   # 隐藏层神经元的数量\n",
    "output_size = 1    # 输出的数量\n",
    "\n",
    "# 实例化模型并将其移动到指定的设备 (CPU 或 GPU)\n",
    "model = SimpleNeuralNetwork(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# 创建一些随机的假数据和标签\n",
    "# 确保数据也移动到与模型相同的设备上\n",
    "num_samples = 100\n",
    "X = torch.randn(num_samples, input_size).to(device)  # 输入数据\n",
    "y = torch.randn(num_samples, output_size).to(device) # 对应的标签\n",
    "\n",
    "# --- 4. 定义损失函数和优化器 ---\n",
    "criterion = nn.MSELoss() # 均方误差损失，适用于回归问题\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01) # 随机梯度下降优化器，学习率为 0.01\n",
    "\n",
    "# --- 5. 训练神经网络 ---\n",
    "print(\"\\n--- 开始训练神经网络 ---\")\n",
    "num_epochs = 100 # 训练轮数\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 将模型设置为训练模式\n",
    "    model.train()\n",
    "\n",
    "    # 前向传播\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "\n",
    "    # 反向传播和优化\n",
    "    optimizer.zero_grad() # 清除之前的梯度\n",
    "    loss.backward()       # 计算梯度\n",
    "    optimizer.step()      # 更新模型参数\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"--- 训练完成 ---\")\n",
    "\n",
    "# --- 6. 评估模型 (可选) ---\n",
    "print(\"\\n--- 模型评估 (在训练数据上) ---\")\n",
    "model.eval() # 将模型设置为评估模式\n",
    "with torch.no_grad(): # 在评估时不计算梯度，节省内存和计算\n",
    "    predicted_outputs = model(X)\n",
    "    eval_loss = criterion(predicted_outputs, y)\n",
    "    print(f\"评估损失 (MSE): {eval_loss.item():.4f}\")\n",
    "\n",
    "# --- 7. 打印学到的参数 (可选) ---\n",
    "# 注意：对于简单的全连接层，可以直接访问权重和偏置\n",
    "print(\"\\n--- 学到的模型参数示例 (第一层权重和偏置) ---\")\n",
    "# 检查fc1层的权重形状\n",
    "print(f\"第一层权重形状: {model.fc1.weight.shape}\")\n",
    "# 打印部分权重和偏置，避免输出过长\n",
    "print(f\"第一层权重示例:\\n{model.fc1.weight.data[:2, :5]}\") # 打印前2行前5列\n",
    "print(f\"第一层偏置示例:\\n{model.fc1.bias.data[:5]}\") # 打印前5个偏置\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Python 3.13)",
   "language": "python",
   "name": "my_python313_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
