{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd76a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个通用的、可定制的全连接神经网络（MLP）类\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_dims):\n",
    "        # 调用父类 nn.Module 的构造函数\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        # 定义一个空的 nn.ModuleList 来存放所有的线性层\n",
    "        self.layers = nn.ModuleList()\n",
    "        # 定义激活函数\n",
    "        self.activation = nn.Tanh() # 也可以使用 nn.ReLU(), nn.Sigmoid() 等\n",
    "        # 根据 layer_dims 列表自动创建所有的线性层\n",
    "        # layer_dims 示例: [2, 32, 32, 1]\n",
    "        # 循环从第一个维度到倒数第二个维度\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            # 创建一个线性层，输入维度为 layer_dims[i]，输出维度为 layer_dims[i+1]\n",
    "            layer = nn.Linear(layer_dims[i], layer_dims[i + 1])\n",
    "            # 将创建的线性层添加到 layers 列表中\n",
    "            self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 这个方法接收输入张量 x。\n",
    "        # 使用一个循环，让 x 依次通过 nn.ModuleList 中的线性层和激活函数。\n",
    "        # 注意：最后一层后面通常没有激活函数，因为我们希望输出是原始的数值。\n",
    "        # 你需要确保你的循环逻辑能正确处理这一点。\n",
    "        \n",
    "        # 遍历除了最后一层之外的所有层\n",
    "        for i, layer in enumerate(self.layers[:-1]): # self.layers[:-1] 表示除了最后一个元素之外的所有元素\n",
    "            x = layer(x) # 数据通过线性层\n",
    "            x = self.activation(x) # 线性层后接激活函数\n",
    "            \n",
    "        # 处理最后一层（通常没有激活函数）\n",
    "        x = self.layers[-1](x) # 数据通过最后一个线性层\n",
    "        \n",
    "        return x # 返回最终输出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71bc107c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MLP 网络构建与测试 ---\n",
      "CUDA 可用，使用 GPU 进行计算\n",
      "网络结构: MLP(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      "  (activation): Tanh()\n",
      ")\n",
      "网络所在的设备: cuda:0\n",
      "\n",
      "输入数据所在的设备: cuda:0\n",
      "输出形状: torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"--- MLP 网络构建与测试 ---\")\n",
    "    # 定义一个层级结构示例：输入2个神经元，两个隐藏层各有32个神经元，输出1个神经元\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA 可用，使用 GPU 进行计算\")\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"CUDA 不可用，使用 CPU 进行计算\")\n",
    "    # 定义网络层级结构\n",
    "    layers = [2, 32, 32, 1]\n",
    "    # 实例化你的 MLP 网络\n",
    "    my_network = MLP(layers)\n",
    "    # 将网络移动到指定的设备（GPU 或 CPU）\n",
    "    my_network.to(device)\n",
    "    # 打印网络结构\n",
    "    print(f\"网络结构: {my_network}\")    \n",
    "    print(f\"网络所在的设备: {next(my_network.parameters()).device}\") # 打印网络参数所在的设备\n",
    "\n",
    "    # 创建一个假的输入张量，形状要匹配网络的输入维度\n",
    "    # 例如，torch.randn(10, 2) 代表10个2维的输入点\n",
    "    dummy_input = torch.randn(10, layers[0]) # layers[0] 是输入维度\n",
    "    dummy_input = dummy_input.to(device) # 将假输入移动到同一设备\n",
    "    print(f\"\\n输入数据所在的设备: {dummy_input.device}\")\n",
    "    # 将假输入送入网络，得到输出\n",
    "    output = my_network(dummy_input) # 取消注释，现在 forward 方法已实现\n",
    "    print(f\"输出形状: {output.shape}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7474f2",
   "metadata": {},
   "source": [
    "| .to(device)理由           | 一句话                                     |\n",
    "| ------------ | --------------------------------------- |\n",
    "| **显式控制**     | 动态图逐行执行，开发者必须明确张量/参数在哪。                 |\n",
    "| **调试透明**     | `device` 不匹配立刻报错，定位简单。                  |\n",
    "| **多 GPU 并行** | 手动放置才能做模型并行、数据并行等高级策略。                  |\n",
    "| **性能优化**     | `.to(device)` 提醒一次性搬数据，避免频繁 CPU↔GPU 拷贝。 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b21ef0",
   "metadata": {},
   "source": [
    "| 构件 / 概念                              | 一句话重点                                              |\n",
    "| ------------------------------------ | -------------------------------------------------- |\n",
    "| `class MLP(nn.Module)`               | 继承 `nn.Module` 获得 PyTorch 模块所有核心能力。                |\n",
    "| `__init__`                           | 构造函数，**自动**初始化网络层、激活函数等结构。                         |\n",
    "| `super().__init__()`                 | **必须**先调用，确保父类完成内部初始化。                             |\n",
    "| `nn.ModuleList()`                    | 用于保存子层，**让 PyTorch 正确追踪参数**。                       |\n",
    "| `nn.Tanh()`                          | **类**，实例化后作为激活函数。                                  |\n",
    "| `for` 循环在 `__init__`                 | 根据 `layer_dims` **动态**堆叠线性层与激活。                    |\n",
    "| `forward(self, x)`                   | 定义前向传播逻辑，**必须实现**。                                 |\n",
    "| `my_network(x)`                      | **无需**显式调用 `forward()`，PyTorch 通过 `__call__` 自动执行。 |\n",
    "| `if __name__ == \"__main__\":`         | 保证测试代码仅在**直接运行**文件时执行。                             |\n",
    "| `torch.randn(batch_size, input_dim)` | 生成**假数据**，快速验证模型初始化与前向传播。                          |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Python 3.13)",
   "language": "python",
   "name": "my_python313_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
